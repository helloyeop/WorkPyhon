{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:JVM is already running. Do not init twice!\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "from konlpy import jvm\n",
    "logger = logging.getLogger(__name__)\n",
    "jvm.init_jvm()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = pd.read_excel(\"data.xlsx\")\n",
    "data_set = data_set.iloc[:,1:]\n",
    "data_set.rename({0:'news'},axis=1, inplace= True)\n",
    "data_label = {1:'culture',2:'global',3:'politic',4:'society',5:'economy'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data_set)) :\n",
    "    data_set['news'][i] = re.sub('[^a-zA-Z0-9 ㄱ-ㅣ가-힣]', '', data_set['news'][i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data_set.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = data_set.iloc[:40000,:]  # 5만 이상 데이터는 오류 발생(왜?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>테슬라 국내 판매 가격 조정최대 5830만원</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이영애 방탄소년단 콘서트 인증샷 공개신곡 기대</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>보복조치 에 부메랑유후인벳푸 호텔3곳 1천100명 예약취소</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>전국 맑고 일교차 큰 날씨안개미세먼지 주의</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정부 해양조사선 독도 항행에 인정 못 해 강력 항의종합</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               news  label\n",
       "0          테슬라 국내 판매 가격 조정최대 5830만원      1\n",
       "1         이영애 방탄소년단 콘서트 인증샷 공개신곡 기대      1\n",
       "2  보복조치 에 부메랑유후인벳푸 호텔3곳 1천100명 예약취소      2\n",
       "3           전국 맑고 일교차 큰 날씨안개미세먼지 주의      1\n",
       "4    정부 해양조사선 독도 항행에 인정 못 해 강력 항의종합      2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun(text):\n",
    "    tokenizer = Twitter()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n",
    "\n",
    "def get_noun2(text):\n",
    "    tokenizer = Hannanum()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n",
    "\n",
    "def get_noun3(text):\n",
    "    tokenizer = Okt()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n",
    "\n",
    "def get_morphs3(text):\n",
    "    tokenizer = Okt()\n",
    "    morphs = tokenizer.morphs(text)\n",
    "    return [n for n in morphs]\n",
    "\n",
    "def get_noun4(text):\n",
    "    tokenizer = Komoran()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data_set, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.reset_index(drop=True)\n",
    "test_set=test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit_transform = CV.fit_transform(train_set[\"news\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label_set = data_set['label']\n",
    "label_set = train_set['label']\n",
    "len(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>베트남 아내 폭행 남성의 티셔츠에 새겨진 말 Stay Humble</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>씨름 이 좋은 걸 할배들만 봤네 열광스포츠 예능 시대</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>이슈레터 역대 대통령 지지율 반토막 만든 결정적 한방은</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그것이 알고 싶다  캘리포니아 살인 사건 용의자 추적</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>강남의 밤을 매수한 01 버닝썬은 픽션이 아니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>나몰라라 파업에르노삼성 부산공장 29일부터 셧다운</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>지소미아 종료 1주일앞대통령 원칙론으로 태도변화 압박</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>동성 연인에서 데이트 폭력 피해자로</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>법조계 수사방해 목적으로 장관 인사권 행사땐 직권남용</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>세계 관광지 점령한 중국인 비매너 퇴치법</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news  label\n",
       "0      베트남 아내 폭행 남성의 티셔츠에 새겨진 말 Stay Humble      4\n",
       "1            씨름 이 좋은 걸 할배들만 봤네 열광스포츠 예능 시대       1\n",
       "2            이슈레터 역대 대통령 지지율 반토막 만든 결정적 한방은      3\n",
       "3             그것이 알고 싶다  캘리포니아 살인 사건 용의자 추적      4\n",
       "4                강남의 밤을 매수한 01 버닝썬은 픽션이 아니다      1\n",
       "...                                     ...    ...\n",
       "27995           나몰라라 파업에르노삼성 부산공장 29일부터 셧다운      1\n",
       "27996         지소미아 종료 1주일앞대통령 원칙론으로 태도변화 압박      3\n",
       "27997                   동성 연인에서 데이트 폭력 피해자로      4\n",
       "27998         법조계 수사방해 목적으로 장관 인사권 행사땐 직권남용      3\n",
       "27999                세계 관광지 점령한 중국인 비매너 퇴치법      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Kkma\n",
    "tf.reset_default_graph()\n",
    "twitter = Twitter()\n",
    "token = []\n",
    "embeddingmodel = []\n",
    "for i in range(len(data_set)):\n",
    "    label = data_set['label']\n",
    "    sentence = twitter.pos(data_set['news'][i], norm=True, stem=True)\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0])\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1])\n",
    "    all_temp.append(temp)\n",
    "    embeddingmodel.append(temp_embedding)\n",
    "    \n",
    "    all_temp.append(label[i])\n",
    "    token.append(all_temp)\n",
    "    \n",
    "embeddingmodel = []\n",
    "for i in range(len(label_set)):\n",
    "    temp_embeddingmodel = []\n",
    "    for k in range(len(token[i][0])):\n",
    "        temp_embeddingmodel.append(token[i][0][k])\n",
    "    embeddingmodel.append(temp_embeddingmodel)\n",
    "embedding  = Word2Vec(embeddingmodel, size = 300, window =5, min_count = 10,\n",
    "                      iter = 5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('신종/Noun', 0.9903484582901001),\n",
       " ('폐렴/Noun', 0.9758926630020142),\n",
       " ('우한/Noun', 0.9711862802505493),\n",
       " ('코로나바이러스/Noun', 0.9571097493171692),\n",
       " ('국내/Noun', 0.9462285041809082),\n",
       " ('확/Noun', 0.9444772005081177),\n",
       " ('전세기/Noun', 0.9389653205871582),\n",
       " ('번째/Suffix', 0.9301178455352783),\n",
       " ('확진/Noun', 0.9293054342269897),\n",
       " ('진자/Noun', 0.9289625883102417)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.most_similar('코로나/Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['테슬라/Noun', '국내/Noun', '판매/Noun', '가격/Noun', '조정/Noun', '최대/Noun', '5830만원/Number']\n",
      "['이영애/Noun', '방탄소년단/Noun', '콘서트/Noun', '인증샷/Noun', '공개/Noun', '신곡/Noun', '기대/Noun']\n",
      "['보복조치/Noun', '에/Josa', '부메랑/Noun', '유후인/Noun', '벳푸/Noun', '호텔/Noun', '3/Number', '곳/Noun', '1천/Number', '100/Number', '명/Noun', '예약/Noun', '취소/Noun']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>테슬라 국내 판매 가격 조정최대 5830만원</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>이영애 방탄소년단 콘서트 인증샷 공개신곡 기대</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>보복조치 에 부메랑유후인벳푸 호텔3곳 1천100명 예약취소</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>전국 맑고 일교차 큰 날씨안개미세먼지 주의</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>정부 해양조사선 독도 항행에 인정 못 해 강력 항의종합</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>야생 토끼 먹었다  또 흑사병 확진중국인 불안감</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>엔드게임 신기록 행진4월  영화 점유율 역대 최저</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>구자경 LG 명예회장 별세25년간 글로벌 초우량 LG 도약 이끌</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>깡통전세 불안감 확산 1억짜리가 8000만원대에 낙찰 경매시장에</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>경호원 기관총에 하태경 섬뜩 교과서적 대응종합</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      news  label\n",
       "0                 테슬라 국내 판매 가격 조정최대 5830만원      1\n",
       "1                이영애 방탄소년단 콘서트 인증샷 공개신곡 기대      1\n",
       "2         보복조치 에 부메랑유후인벳푸 호텔3곳 1천100명 예약취소      2\n",
       "3                  전국 맑고 일교차 큰 날씨안개미세먼지 주의      1\n",
       "4           정부 해양조사선 독도 항행에 인정 못 해 강력 항의종합      2\n",
       "...                                    ...    ...\n",
       "39995           야생 토끼 먹었다  또 흑사병 확진중국인 불안감      2\n",
       "39996          엔드게임 신기록 행진4월  영화 점유율 역대 최저      1\n",
       "39997  구자경 LG 명예회장 별세25년간 글로벌 초우량 LG 도약 이끌      5\n",
       "39998  깡통전세 불안감 확산 1억짜리가 8000만원대에 낙찰 경매시장에      5\n",
       "39999            경호원 기관총에 하태경 섬뜩 교과서적 대응종합      3\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token[0][0])\n",
    "print(token[1][0])\n",
    "print(token[2][0])\n",
    "data_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def One_hot(data):\n",
    "       \n",
    "    index_dict = {value:index for index,value in enumerate(set(data))}\n",
    "    result = []\n",
    "\n",
    "    for value in data:\n",
    "\n",
    "        one_hot = np.zeros(len(index_dict))\n",
    "        index = index_dict[value]\n",
    "        one_hot[index] = 1\n",
    "        result.append(one_hot)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def Convert2Vec(model_name, doc): \n",
    "    #train_X_ = W2V.Convert2Vec(\"Word2Vec_csv_article.embedding\",train_X)\n",
    "    word_vec = []\n",
    "    model = model_name\n",
    "    for sent in doc:\n",
    "        sub = []\n",
    "        for word in sent:\n",
    "            if word in model.wv.vocab:\n",
    "                sub.append(model.wv[word]) \n",
    "            else:\n",
    "                sub.append(np.random.uniform(-0.25,0.25,300)) # 사전에 해당 워드가 없으면 랜덤한 vector 생성하게 해줌\n",
    "        word_vec.append(sub)\n",
    "\n",
    "    return word_vec\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_LSTM():\n",
    "    \n",
    "    def __init__(self, lstm_units, num_class, keep_prob):\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('backward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('Weights', reuse = tf.AUTO_REUSE):\n",
    "           \n",
    "            self.W = tf.get_variable(name=\"W\", shape=[2 * lstm_units, num_class],\n",
    "                                dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
    "            self.b = tf.get_variable(name=\"b\", shape=[num_class], dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            \n",
    "            \n",
    "    def logits(self, X, W, b, seq_len):\n",
    "        \n",
    "        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, self.lstm_bw_cell,dtype=tf.float32,\n",
    "                                                                            inputs = X, sequence_length = seq_len)\n",
    "\n",
    "        outputs = tf.concat([states[0][1], states[1][1]], axis=1)\n",
    "        pred = tf.matmul(outputs, W) + b        \n",
    "        return pred\n",
    "        \n",
    "    def model_build(self, logits, labels, learning_rate = 0.001):\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits , labels = labels)) # Softmax loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) \n",
    "            merged = tf.summary.merge_all()\n",
    "        return loss, optimizer, merged\n",
    "    \n",
    "    def graph_build(self, avg_loss, avg_acc):\n",
    "        \n",
    "        tf.summary.scalar('Loss', avg_loss)\n",
    "        tf.summary.scalar('Accuracy', avg_acc)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size):\n",
    "        \n",
    "    zero_pad = np.zeros((Batch_size, Maxseq_length, Vector_size))\n",
    "    for i in range(Batch_size):\n",
    "        zero_pad[i, :np.shape(train_batch_X[i])[0], :np.shape(train_batch_X[i])[1] ] = train_batch_X[i]\n",
    "    return zero_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000\n",
      "28000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "tokens = np.array(token)\n",
    "data_x = tokens[:,0]\n",
    "data_y = tokens[:,1]\n",
    "\n",
    "\n",
    "data_y = One_hot(data_y)\n",
    "data_x = Convert2Vec(embedding, data_x)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x,data_y,\n",
    "                                                    test_size = 0.3,\n",
    "                                                    random_state = 42)\n",
    "print(len(train_x))\n",
    "print(len(train_y))\n",
    "print(len(test_x))\n",
    "#train_x = train_x.reset_index(drop=True)\n",
    "#train_y = train_y.reset_index(drop=True)\n",
    "#test_x = test_x.reset_index(drop=True)\n",
    "#test_y = test_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x000002620C25BFC8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Start training!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "Batch_size = 32\n",
    "Total_size = len(train_x)\n",
    "Vector_size = 300\n",
    "seq_length = [len(x) for x in train_x]\n",
    "Maxseq_length = max(seq_length)\n",
    "learning_rate = 0.001\n",
    "lstm_units = 128\n",
    "num_class = 5\n",
    "training_epochs = 5\n",
    "keep_prob = 0.75\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, num_class])\n",
    "seq_len = tf.placeholder(tf.int32 , shape = [None])\n",
    "\n",
    "BiLSTM = Bi_LSTM(lstm_units, num_class, keep_prob)\n",
    "with tf.variable_scope('loss', reuse = tf.AUTO_REUSE) :\n",
    "    logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len)\n",
    "    loss, optimizer, merged = BiLSTM.model_build(logits, Y, learning_rate)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "total_batch = int(Total_size / Batch_size)\n",
    "\n",
    "print(\"Start training!\")\n",
    "\n",
    "#modelName = \"BiLSTM.model\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>제 발등 찍은 일본대일 무역적자 16년만에 최저치 전망</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>잠적 북한외교관 딸 북송 이탈리아 정가에 파문종합2보</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>정권 애완견 하다 의원 되는 게 평범한 정의냐 진중권 여당행</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>자식 넷 모두 명문 의대 보낸 거실 공부가 비결</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>블루보틀 상륙에 울고 웃는 사람들</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>날씨 포근한 초봄 날씨에 미세먼지 내일 출근길 비눈</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>단독 윤석열 대학살 충격측근들 불러 해야할 일 했다</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>속보 문 대통령 조국 사태 후 윤석열 총장과 첫 대면윤 총장 거</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>WHO 중국에 국제 전문가 파견 합의시진핑 악마와 싸움 이길</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>밀착마크김부겸 새마을 척결이 훈장 이게 진보 넌센스</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      news  label\n",
       "0           제 발등 찍은 일본대일 무역적자 16년만에 최저치 전망      5\n",
       "1            잠적 북한외교관 딸 북송 이탈리아 정가에 파문종합2보      2\n",
       "2       정권 애완견 하다 의원 되는 게 평범한 정의냐 진중권 여당행       3\n",
       "3               자식 넷 모두 명문 의대 보낸 거실 공부가 비결      4\n",
       "4                       블루보틀 상륙에 울고 웃는 사람들      5\n",
       "...                                    ...    ...\n",
       "11995         날씨 포근한 초봄 날씨에 미세먼지 내일 출근길 비눈      1\n",
       "11996         단독 윤석열 대학살 충격측근들 불러 해야할 일 했다      4\n",
       "11997  속보 문 대통령 조국 사태 후 윤석열 총장과 첫 대면윤 총장 거      3\n",
       "11998    WHO 중국에 국제 전문가 파견 합의시진핑 악마와 싸움 이길      2\n",
       "11999         밀착마크김부겸 새마을 척결이 훈장 이게 진보 넌센스      3\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             제 발등 찍은 일본대일 무역적자 16년만에 최저치 전망\n",
      "1              잠적 북한외교관 딸 북송 이탈리아 정가에 파문종합2보\n",
      "2         정권 애완견 하다 의원 되는 게 평범한 정의냐 진중권 여당행 \n",
      "3                 자식 넷 모두 명문 의대 보낸 거실 공부가 비결\n",
      "4                         블루보틀 상륙에 울고 웃는 사람들\n",
      "                        ...                 \n",
      "11995           날씨 포근한 초봄 날씨에 미세먼지 내일 출근길 비눈\n",
      "11996           단독 윤석열 대학살 충격측근들 불러 해야할 일 했다\n",
      "11997    속보 문 대통령 조국 사태 후 윤석열 총장과 첫 대면윤 총장 거\n",
      "11998      WHO 중국에 국제 전문가 파견 합의시진핑 악마와 싸움 이길\n",
      "11999           밀착마크김부겸 새마을 척결이 훈장 이게 진보 넌센스\n",
      "Name: news, Length: 12000, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['보복조치/Noun',\n",
       " '에/Josa',\n",
       " '부메랑/Noun',\n",
       " '유후인/Noun',\n",
       " '벳푸/Noun',\n",
       " '호텔/Noun',\n",
       " '3/Number',\n",
       " '곳/Noun',\n",
       " '1천/Number',\n",
       " '100/Number',\n",
       " '명/Noun',\n",
       " '예약/Noun',\n",
       " '취소/Noun']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_set['news'])\n",
    "tokens[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-d069975a0b47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mtrain_batch_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBatch_size\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mbatch_seq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBatch_size\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mtrain_batch_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZero_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[1;31m#summary, _ = sess.run([merged,optimizer], feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-3ca837d7e8e5>\u001b[0m in \u001b[0;36mZero_padding\u001b[1;34m(train_batch_X, Batch_size, Maxseq_length, Vector_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mzero_pad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVector_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mzero_pad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mzero_pad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "result  = []\n",
    "loss_graph = []\n",
    "loss_graph_test=[]\n",
    "with tf.Session() as sess:\n",
    "    #writer = tf.summary.FileWriter('', sess.graph)\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('Bidirectional_LSTM', sess.graph)\n",
    "    i = 0\n",
    "    for epoch in range(training_epochs):\n",
    "        #try :\n",
    "            avg_acc, avg_loss = 0. , 0.\n",
    "            for step in range(total_batch):\n",
    "\n",
    "                train_batch_X = train_x[step*Batch_size : step*Batch_size+Batch_size]\n",
    "\n",
    "                train_batch_Y = train_y[step*Batch_size : step*Batch_size+Batch_size]\n",
    "                batch_seq_length = seq_length[step*Batch_size : step*Batch_size+Batch_size]\n",
    "                train_batch_X = Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "                \n",
    "                #summary, _ = sess.run([merged,optimizer], feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "\n",
    "                sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "                \n",
    "                loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "                avg_loss += loss_ / total_batch\n",
    "\n",
    "                acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "                avg_acc += acc / total_batch\n",
    "                #train_writer.add_summary(summary, step)\n",
    "            print(\"epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}\".format(epoch+1, step+1, loss_, acc))\n",
    "            #print(\"Test Accuracy : \", sess.run(accuracy, feed_dict={X:test_x, Y:test_y}))\n",
    "            #summary = sess.run(BiLSTM.graph_build(avg_loss, avg_acc))       \n",
    "            #train_writer.add_summary(summary, i)\n",
    "            #merged = BiLSTM.graph_build()\n",
    "            i += 1\n",
    "        #except : \n",
    "            #print('Error Epoch : ', epoch)\n",
    "            loss_graph.append([acc,epoch])\n",
    "            #acc = sess.run(accuracy , feed_dict={X: test_x, Y: test_y, seq_len: batch_seq_length})\n",
    "            #print('Test Accuracy : ', acc)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    minute = int(duration / 60)\n",
    "    second = int(duration) % 60\n",
    "    print(\"%dminutes %dseconds\" % (minute,second))\n",
    "    save_path = saver.save(sess, os.getcwd())\n",
    "\n",
    "    train_writer.close()\n",
    "    print('save_path',save_path)\n",
    "    \n",
    "    test_size = len(test_x)\n",
    "    test_batch = int(test_size / Batch_size)\n",
    "    seq_length = [len(x) for x in test_x]\n",
    "    keep_prob = 1.0\n",
    "    total_acc = 0\n",
    "    for step in range(test_batch):\n",
    "        test_batch_X = test_x[step*Batch_size: step*Batch_size + Batch_size]\n",
    "        test_batch_Y = test_y[step*Batch_size: step*Batch_size + Batch_size]\n",
    "        batch_seq_length = seq_length[step*Batch_size : step*Batch_size + Batch_size]\n",
    "        test_batch_X = Zero_padding(test_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "        \n",
    "        #summary, _ = sess.run([mreged, optimizer], feed_dict = {X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    "        acc = sess.run(accuracy , feed_dict={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    "        total_acc += acc/test_batch\n",
    "        pred_v = sess.run(prediction , feed_dict={X: test_batch_X, Y: test_batch_Y, seq_len: batch_seq_length})\n",
    " \n",
    "        print(\"loss = {:.6f} accuracy = {:.6f}\".format(loss_, acc), \"/ step =\",step)\n",
    "            \n",
    "            \n",
    "    print(\" Total Accuracy : {}\".format(total_acc))\n",
    "    result.append(total_acc)\n",
    "\n",
    "    \n",
    "print(\"----------------------------------------\")\n",
    "print(\"BI-LSTM finish\")\n",
    "for i in range(0, len(result)):\n",
    "    print(\"epoch : \", i+1, \" / accuracy : \", result[i])\n",
    "    \n",
    "print(\"\\nBest Result epoch : \", result.index(max(result)) + 1, \" / accuracy: \", max(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
