{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Bi_LSTM():\n",
    "    \n",
    "    def __init__(self, lstm_units, num_class, keep_prob):\n",
    "        \n",
    "        self.lstm_units = lstm_units\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "            \n",
    "        with tf.variable_scope('backward', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            self.lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
    "            self.lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
    "        \n",
    "        with tf.variable_scope('Weights', reuse = tf.AUTO_REUSE):\n",
    "           \n",
    "            self.W = tf.get_variable(name=\"W\", shape=[2 * lstm_units, num_class],\n",
    "                                dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
    "            self.b = tf.get_variable(name=\"b\", shape=[num_class], dtype=tf.float32,\n",
    "                                initializer=tf.zeros_initializer())\n",
    "            \n",
    "            \n",
    "    def logits(self, X, W, b, seq_len):\n",
    "        \n",
    "        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, self.lstm_bw_cell,dtype=tf.float32,\n",
    "                                                                            inputs = X, sequence_length = seq_len)\n",
    "        ## concat fw, bw final states\n",
    "        outputs = tf.concat([states[0][1], states[1][1]], axis=1)\n",
    "        pred = tf.matmul(outputs, W) + b        \n",
    "        return pred\n",
    "        \n",
    "    def model_build(self, logits, labels, learning_rate = 0.001):\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits , labels = labels)) # Softmax loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # Adam Optimizer\n",
    "            \n",
    "        return loss, optimizer\n",
    "    \n",
    "    def graph_build(self, avg_loss, avg_acc):\n",
    "        \n",
    "        tf.summary.scalar('Loss', avg_loss)\n",
    "        tf.summary.scalar('Accuracy', avg_acc)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "from gensim.models import Word2Vec\n",
    "import csv\n",
    "\n",
    "\"\"\"\n",
    "@author: lumyjuwon\n",
    "\"\"\"\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "file = open(\"Article_shuffled.csv\", 'r', encoding='euc-kr')\n",
    "line = csv.reader(file)\n",
    "token = []\n",
    "embeddingmodel = []\n",
    "\n",
    "for i in line:\n",
    "    content = i[3]  # csv에서 뉴스 제목 또는 뉴스 본문 column으로 변경\n",
    "    sentence = twitter.pos(i[0], norm=True, stem=True)\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0])\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1])\n",
    "    all_temp.append(temp)\n",
    "    embeddingmodel.append(temp_embedding)\n",
    "    category = i[1]  # csv에서 category column으로 변경\n",
    "    category_number_dic = {'IT과학': 0, '경제': 1, '정치': 2, 'e스포츠': 3, '골프': 4, '농구': 5, '배구': 6, '야구': 7, '일반 스포츠': 8, '축구': 9, '사회': 10, '생활문화': 11}\n",
    "    all_temp.append(category_number_dic.get(category))\n",
    "    token.append(all_temp)\n",
    "print(\"토큰 처리 완료\")\n",
    "\n",
    "\n",
    "embeddingmodel = []\n",
    "for i in range(len(token)):\n",
    "    temp_embeddingmodel = []\n",
    "    for k in range(len(token[i][0])):\n",
    "        temp_embeddingmodel.append(token[i][0][k])\n",
    "    embeddingmodel.append(temp_embeddingmodel)\n",
    "embedding = Word2Vec(embeddingmodel, size=300, window=5, min_count=10, iter=5, sg=1, max_vocab_size=360000000)\n",
    "embedding.save('post.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "\n",
    "class Word2Vec():\n",
    "    \n",
    "    def __init__(self):\n",
    "        None\n",
    "\n",
    "    def tokenize(self, doc):\n",
    "        pos_tagger = Twitter()\n",
    "        return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "    \n",
    "    def read_data(self, filename):\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "            data = data[1:]\n",
    "        return data  \n",
    "    \n",
    "    def Word2vec_model(self, model_name):\n",
    "        \n",
    "        model = gensim.models.word2vec.Word2Vec.load(model_name)\n",
    "        return model\n",
    "    \n",
    "    def Convert2Vec(self, model_name, doc): # Convert corpus into vectors\n",
    "        #train_X_ = W2V.Convert2Vec(\"Word2Vec_csv_article.embedding\",train_X)\n",
    "        word_vec = []\n",
    "        model = gensim.models.word2vec.Word2Vec.load(model_name)\n",
    "        for sent in doc:\n",
    "            sub = []\n",
    "            for word in sent:\n",
    "                if word in model.wv.vocab:\n",
    "                    sub.append(model.wv[word]) # Word Vector Input\n",
    "                else:\n",
    "                    sub.append(np.random.uniform(-0.25,0.25,300)) # used for OOV words\n",
    "            word_vec.append(sub)\n",
    "        \n",
    "        return word_vec\n",
    "    \n",
    "    def Zero_padding(self, train_batch_X, Batch_size, Maxseq_length, Vector_size):\n",
    "        \n",
    "        zero_pad = np.zeros((Batch_size, Maxseq_length, Vector_size))\n",
    "        for i in range(Batch_size):\n",
    "            zero_pad[i,:np.shape(train_batch_X[i])[0],:np.shape(train_batch_X[i])[1]] = train_batch_X[i]\n",
    "        return zero_pad\n",
    "    \n",
    "    def One_hot(self, data):\n",
    "       \n",
    "        index_dict = {value:index for index,value in enumerate(set(data))}\n",
    "        result = []\n",
    "        \n",
    "        for value in data:\n",
    "            \n",
    "            one_hot = np.zeros(len(index_dict))\n",
    "            index = index_dict[value]\n",
    "            one_hot[index] = 1\n",
    "            result.append(one_hot)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Bi_LSTM as Bi_LSTM\n",
    "import Word2Vec as Word2Vec\n",
    "import csv\n",
    "from konlpy.tag import Twitter\n",
    "import os\n",
    "\n",
    "twitter = Twitter()\n",
    "W2V = Word2Vec.Word2Vec()\n",
    "\n",
    "file = open(\"Article_shuffled.csv\", 'r', encoding='euc-kr')\n",
    "line = csv.reader(file)\n",
    "token = []\n",
    "embeddingmodel = []\n",
    "\n",
    "for i in line:\n",
    "    content = i[3]  # csv에서 뉴스 제목 또는 뉴스 본문 column으로 변경\n",
    "    sentence = twitter.pos(i[0], norm=True, stem=True)\n",
    "    temp = []\n",
    "    temp_embedding = []\n",
    "    all_temp = []\n",
    "    for k in range(len(sentence)):\n",
    "        temp_embedding.append(sentence[k][0])\n",
    "        temp.append(sentence[k][0] + '/' + sentence[k][1])\n",
    "    all_temp.append(temp)\n",
    "    embeddingmodel.append(temp_embedding)\n",
    "    category = i[1]  # csv에서 category column으로 변경\n",
    "    category_number_dic = {'IT과학': 0, '경제': 1, '정치': 2, 'e스포츠': 3, '골프': 4, '농구': 5, '배구': 6, '야구': 7, '일반 스포츠': 8, '축구': 9, '사회': 10, '생활문화': 11}\n",
    "    all_temp.append(category_number_dic.get(category))\n",
    "    token.append(all_temp)\n",
    "print(\"토큰 처리 완료\")\n",
    "\n",
    "tokens = np.array(token)\n",
    "print(\"token 처리 완료\")\n",
    "print(\"train_data 최신 버전인지 확인\")\n",
    "train_X = tokens[:, 0]\n",
    "train_Y = tokens[:, 1]\n",
    "\n",
    "train_Y_ = W2V.One_hot(train_Y)  # Convert to One-hot\n",
    "train_X_ = W2V.Convert2Vec(\"Data\\\\post.embedding\",train_X)  # import word2vec model where you have trained before\n",
    "\n",
    "Batch_size = 32\n",
    "Total_size = len(train_X)\n",
    "Vector_size = 300\n",
    "seq_length = [len(x) for x in train_X]\n",
    "Maxseq_length = max(seq_length)\n",
    "learning_rate = 0.001\n",
    "lstm_units = 128\n",
    "num_class = 12\n",
    "training_epochs = 5\n",
    "keep_prob = 0.75\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, shape = [None, num_class], name = 'Y')\n",
    "seq_len = tf.placeholder(tf.int32, shape = [None])\n",
    "\n",
    "BiLSTM = Bi_LSTM.Bi_LSTM(lstm_units, num_class, keep_prob)\n",
    "\n",
    "with tf.variable_scope(\"loss\", reuse = tf.AUTO_REUSE):\n",
    "    logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len)\n",
    "    loss, optimizer = BiLSTM.model_build(logits, Y, learning_rate)\n",
    "\n",
    "prediction = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "total_batch = int(Total_size / Batch_size)\n",
    "\n",
    "print(\"Start training!\")\n",
    "\n",
    "modelName = \"BiLSTM.model\"\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('Bidirectional_LSTM', sess.graph)\n",
    "    i = 0\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        avg_acc, avg_loss = 0. , 0.\n",
    "        for step in range(total_batch):\n",
    "\n",
    "            train_batch_X = train_X_[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            train_batch_Y = train_Y_[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            batch_seq_length = seq_length[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            \n",
    "            train_batch_X = W2V.Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            # Compute average loss\n",
    "            loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            avg_loss += loss_ / total_batch\n",
    "            \n",
    "            acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            avg_acc += acc / total_batch\n",
    "            print(\"epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}\".format(epoch+1, step+1, loss_, acc))\n",
    "\n",
    "        summary = sess.run(BiLSTM.graph_build(avg_loss, avg_acc))       \n",
    "        train_writer.add_summary(summary, i)\n",
    "        i += 1\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    minute = int(duration / 60)\n",
    "    second = int(duration) % 60\n",
    "    print(\"%dminutes %dseconds\" % (minute,second))\n",
    "    save_path = saver.save(sess, os.getcwd())\n",
    "\n",
    "    train_writer.close()\n",
    "    print('save_path',save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
