{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-58bb93bec2c6>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./mnist/data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data\", one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28*28\n",
    "n_noise = 128\n",
    "X = tf.placeholder(tf.float32, [None, n_input]) # 실제 디이터 입력\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise]) # 노이즈 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n:noise : 128  / n_hidden : 256 => 128x256\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev = 0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "\n",
    "# 256 x 784\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev = 0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))\n",
    "\n",
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev = 0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden,1], stddev = 0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성기\n",
    "# cost function : 확률적 함수\n",
    "# kullback-Leibler( KL-Divergence) : GAN, VAE  :: 분포의 차이를 확인하는 함수\n",
    "#  * MSE , Entropy : 분류\n",
    "# 생성기 : noiser가 input데이터, cost function : KL-Divergence\n",
    "\n",
    "def generator(noise_z) :  # 노이즈가 128x128로 들어옴\n",
    "    hidden = tf.nn.relu( tf.matmul(noise_z , G_W1 ) + G_b1) # 128 x 256\n",
    "    output = tf.nn.sigmoid( tf.matmul(hidden, G_W2) + G_b2) # 128 x 784\n",
    "    return output # 784 사이즈의 이미지 생성\n",
    "\n",
    "\n",
    "#판별기\n",
    "def discriminator(inputs) 00000000000000000000000000000000000000\n",
    ":   # 실제 이미지의 분포를 확인\n",
    "    hidden = tf.nn.relu( tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid( tf.matmul(hidden, D_W2) + D_b2)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size = (batch_size , n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(Z)  # 노이즈로 이미지 생성\n",
    "D_gene = discriminator(G) # 노이즈로 생성돤 이미지를 판별( 분포를 확인 )\n",
    "D_real = discriminator(X) # 실제 이미지의 분포를 확인\n",
    "\n",
    "# log 사용 이유 : 확률값에 log를 취함으로서 정보량을 구함\n",
    "# => 확률이 높아지면 정보량이 작아짐\n",
    "# => 확률이 낮아지면 정보량이 커짐     ==> tfidf의 논리와 유사 \n",
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1- D_gene))\n",
    "loss_G = tf.reduce_mean(tf.log(D_gene)) # 최대우도 추정법으로 가장 적합한 분포가 될 수 있도록 함\n",
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]\n",
    "# 최대화하기 위해\n",
    "# 역전파 변수를 지정\n",
    "# 판별기의 loss와 생성기의 loss를 minimize\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list) \n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)\n",
    "\n",
    "%matplotlib inline\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "loss_val_D , loss_val_G = 0 ,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0000 D loss : -0.7055 G loss : -1.866\n",
      "Epoch :  0001 D loss : -0.1817 G loss : -2.844\n",
      "Epoch :  0002 D loss : -0.2073 G loss : -2.84\n",
      "Epoch :  0003 D loss : -0.5438 G loss : -1.68\n",
      "Epoch :  0004 D loss : -0.4954 G loss : -1.51\n",
      "Epoch :  0005 D loss : -0.2321 G loss : -2.475\n",
      "Epoch :  0006 D loss : -0.1313 G loss : -3.084\n",
      "Epoch :  0007 D loss : -0.187 G loss : -3.162\n",
      "Epoch :  0008 D loss : -0.2324 G loss : -2.523\n",
      "Epoch :  0009 D loss : -0.3179 G loss : -2.584\n",
      "Epoch :  0010 D loss : -0.3462 G loss : -2.635\n",
      "Epoch :  0011 D loss : -0.459 G loss : -1.96\n",
      "Epoch :  0012 D loss : -0.5327 G loss : -2.013\n",
      "Epoch :  0013 D loss : -0.401 G loss : -2.281\n",
      "Epoch :  0014 D loss : -0.3553 G loss : -2.415\n",
      "Epoch :  0015 D loss : -0.3086 G loss : -2.583\n",
      "Epoch :  0016 D loss : -0.3976 G loss : -2.575\n",
      "Epoch :  0017 D loss : -0.4277 G loss : -2.805\n",
      "Epoch :  0018 D loss : -0.2863 G loss : -2.861\n",
      "Epoch :  0019 D loss : -0.3389 G loss : -2.896\n",
      "Epoch :  0020 D loss : -0.3075 G loss : -2.973\n",
      "Epoch :  0021 D loss : -0.3657 G loss : -2.638\n",
      "Epoch :  0022 D loss : -0.3614 G loss : -2.879\n",
      "Epoch :  0023 D loss : -0.3606 G loss : -2.753\n",
      "Epoch :  0024 D loss : -0.3065 G loss : -2.787\n",
      "Epoch :  0025 D loss : -0.4228 G loss : -2.908\n",
      "Epoch :  0026 D loss : -0.4689 G loss : -2.902\n",
      "Epoch :  0027 D loss : -0.4578 G loss : -2.274\n",
      "Epoch :  0028 D loss : -0.4368 G loss : -2.544\n",
      "Epoch :  0029 D loss : -0.4851 G loss : -2.567\n",
      "Epoch :  0030 D loss : -0.4167 G loss : -2.705\n",
      "Epoch :  0031 D loss : -0.4227 G loss : -2.767\n",
      "Epoch :  0032 D loss : -0.4946 G loss : -2.423\n",
      "Epoch :  0033 D loss : -0.4705 G loss : -2.915\n",
      "Epoch :  0034 D loss : -0.3857 G loss : -2.606\n",
      "Epoch :  0035 D loss : -0.55 G loss : -2.583\n",
      "Epoch :  0036 D loss : -0.507 G loss : -2.868\n",
      "Epoch :  0037 D loss : -0.5058 G loss : -2.588\n",
      "Epoch :  0038 D loss : -0.515 G loss : -2.736\n",
      "Epoch :  0039 D loss : -0.4565 G loss : -2.253\n",
      "Epoch :  0040 D loss : -0.4188 G loss : -2.355\n",
      "Epoch :  0041 D loss : -0.6406 G loss : -2.556\n",
      "Epoch :  0042 D loss : -0.476 G loss : -2.439\n",
      "Epoch :  0043 D loss : -0.6292 G loss : -2.218\n",
      "Epoch :  0044 D loss : -0.6976 G loss : -2.512\n",
      "Epoch :  0045 D loss : -0.6503 G loss : -2.894\n",
      "Epoch :  0046 D loss : -0.5896 G loss : -2.261\n",
      "Epoch :  0047 D loss : -0.6477 G loss : -2.069\n",
      "Epoch :  0048 D loss : -0.7172 G loss : -2.263\n",
      "Epoch :  0049 D loss : -0.7172 G loss : -2.434\n",
      "Epoch :  0050 D loss : -0.7193 G loss : -2.161\n",
      "Epoch :  0051 D loss : -0.5369 G loss : -2.163\n",
      "Epoch :  0052 D loss : -0.5072 G loss : -2.202\n",
      "Epoch :  0053 D loss : -0.6124 G loss : -2.454\n",
      "Epoch :  0054 D loss : -0.6139 G loss : -2.44\n",
      "Epoch :  0055 D loss : -0.6399 G loss : -2.208\n",
      "Epoch :  0056 D loss : -0.6906 G loss : -2.043\n",
      "Epoch :  0057 D loss : -0.7502 G loss : -2.136\n",
      "Epoch :  0058 D loss : -0.7483 G loss : -2.246\n",
      "Epoch :  0059 D loss : -0.7336 G loss : -1.794\n",
      "Epoch :  0060 D loss : -0.7064 G loss : -1.982\n",
      "Epoch :  0061 D loss : -0.8132 G loss : -2.089\n",
      "Epoch :  0062 D loss : -0.6908 G loss : -2.064\n",
      "Epoch :  0063 D loss : -0.678 G loss : -1.954\n",
      "Epoch :  0064 D loss : -0.7498 G loss : -1.799\n",
      "Epoch :  0065 D loss : -0.756 G loss : -1.76\n",
      "Epoch :  0066 D loss : -0.6556 G loss : -1.97\n",
      "Epoch :  0067 D loss : -0.5404 G loss : -2.11\n",
      "Epoch :  0068 D loss : -0.6375 G loss : -2.188\n",
      "Epoch :  0069 D loss : -0.7031 G loss : -1.95\n",
      "Epoch :  0070 D loss : -0.6041 G loss : -2.249\n",
      "Epoch :  0071 D loss : -0.6329 G loss : -1.806\n",
      "Epoch :  0072 D loss : -0.8109 G loss : -1.927\n",
      "Epoch :  0073 D loss : -0.7812 G loss : -1.693\n",
      "Epoch :  0074 D loss : -0.695 G loss : -2.109\n",
      "Epoch :  0075 D loss : -0.8041 G loss : -1.849\n",
      "Epoch :  0076 D loss : -0.6421 G loss : -1.909\n",
      "Epoch :  0077 D loss : -0.7357 G loss : -1.893\n",
      "Epoch :  0078 D loss : -0.869 G loss : -1.971\n",
      "Epoch :  0079 D loss : -0.8334 G loss : -2.116\n",
      "Epoch :  0080 D loss : -0.8365 G loss : -1.919\n",
      "Epoch :  0081 D loss : -0.618 G loss : -2.162\n",
      "Epoch :  0082 D loss : -0.6761 G loss : -2.095\n",
      "Epoch :  0083 D loss : -0.7253 G loss : -2.038\n",
      "Epoch :  0084 D loss : -0.9689 G loss : -1.817\n",
      "Epoch :  0085 D loss : -0.8823 G loss : -2.033\n",
      "Epoch :  0086 D loss : -0.7213 G loss : -2.044\n",
      "Epoch :  0087 D loss : -0.7375 G loss : -1.949\n",
      "Epoch :  0088 D loss : -0.7924 G loss : -1.857\n",
      "Epoch :  0089 D loss : -0.7119 G loss : -2.073\n",
      "Epoch :  0090 D loss : -0.9615 G loss : -1.899\n",
      "Epoch :  0091 D loss : -0.7256 G loss : -1.879\n",
      "Epoch :  0092 D loss : -0.6695 G loss : -2.006\n",
      "Epoch :  0093 D loss : -0.7656 G loss : -1.716\n",
      "Epoch :  0094 D loss : -0.74 G loss : -1.96\n",
      "Epoch :  0095 D loss : -0.7425 G loss : -2.066\n",
      "Epoch :  0096 D loss : -0.6354 G loss : -1.971\n",
      "Epoch :  0097 D loss : -0.756 G loss : -1.682\n",
      "Epoch :  0098 D loss : -0.664 G loss : -1.865\n",
      "Epoch :  0099 D loss : -0.8472 G loss : -1.613\n",
      "최적화 완료\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "for epoch in range(total_epoch) :\n",
    "    for i in range(total_batch) :\n",
    "        batch_xs , batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise) # 128x128\n",
    "        # 실제 이미지 분포를 확인하기 위함\n",
    "        _, loss_val_D = sess.run([train_D, loss_D],\n",
    "                                feed_dict = {X: batch_xs, Z: noise})\n",
    "        # 아마자룰 생성( noise : 128x128)\n",
    "        _, loss_val_G = sess.run([train_G, loss_G],\n",
    "                                feed_dict={Z: noise})\n",
    "    print(\"Epoch : \", '%04d' % epoch,\n",
    "         'D loss : {:.4}'.format(loss_val_D),\n",
    "         \"G loss : {:.4}\".format(loss_val_G))\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        # 노이즈로부터 이미지를 생성 : generator\n",
    "        noise = get_noise(sample_size , n_noise)   #10 x 128 개의 노이즈\n",
    "        samples = sess.run(G, feed_dict={Z: noise}) \n",
    "        fig, ax =  plt.subplots(1, sample_size , figsize=(sample_size,1))\n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()     # 128 => 784 이미지 사이즈로 생성\n",
    "            ax[i].imshow(np.reshape(samples[i], (28,28)))\n",
    "        plt.savefig('samples/{}.png'.format(str(epoch).zfill(3)),\n",
    "                   bbox_inches = 'tight')\n",
    "        plt.close(fig)\n",
    "print(\"최적화 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
