{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B3A86348>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B3A86348>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B3A86348>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B3A86348>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B417FF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B417FF08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B417FF08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x00000286B417FF08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000286B3B0FE08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000286B3B0FE08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000286B3B0FE08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000286B3B0FE08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# 데이터가 한글자씩 들어감\n",
    "char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz단어나무놀이소녀연습사랑'] \n",
    "                                \n",
    "num_dic =  {n: i for i , n  in enumerate(char_arr)} #enumerate : 각 데이터에 번호를 달아줌\n",
    "dic_len = len(num_dic)\n",
    "print(dic_len)\n",
    "seq_data =[['word','단어'],['wood','나무'],\n",
    "           ['game','놀이'],['girl','소녀'],\n",
    "           ['test','연습'],['love','사랑']]\n",
    "#seq2seq 번역망\n",
    "\n",
    "def make_batch(seq_data) :\n",
    "    \n",
    "    input_batch = []\n",
    "    output_batch = []\n",
    "    target_batch = []\n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0]]          # word => num_dic에서 번호로 바뀜 25,17,..\n",
    "        \n",
    "        # 첫번째 입력 데이터는 'word'이 학습된 smoking gun이 들어옴 \n",
    "        output = [num_dic[n] for n in (\"S\" + seq[1])] # Start의 S  => 시작을 확인하기 위해서\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')] # End => 끝을 확인하기 위해서( 글자 수가 다른 경우, 인덱스로 지정이 어렵기 떄문에 E를 끝으로 설정하여 출력)\n",
    "        \n",
    "        input_batch.append(np.eye(dic_len)[input]) # 42개 대각행렬( 단위행렬(identity matrix)) => one-hot 인코딩\n",
    "        # 숫자로된 word가 one-hot되어 4x41가 만들어짐\n",
    "        \n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        # 2글자이므로 one-hot되어 2x41\n",
    "        \n",
    "        target_batch.append(target)\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128    # neurons 출력이 128차\n",
    "total_epoch = 100\n",
    "n_class = n_input = dic_len\n",
    "\n",
    "# encoder, decoder용 데이터 주입 변수\n",
    "enc_input = tf.placeholder(tf.float32, [None,None,n_input])   \n",
    "dec_input = tf.placeholder(tf.float32, [None,None,n_input])\n",
    "targets = tf.placeholder(tf.int64, [None,None])\n",
    "with tf.variable_scope('encode'):\n",
    "    # FFNN 한개 => 입력 -> 가중치 ---> 출력(n_hidden)\n",
    "    enc_cell =tf.nn.rnn_cell.BasicRNNCell(n_hidden)   # 입력 41 x 128 \n",
    "    \n",
    "    # 과적합 방지를 위해 계산회로 생략 - random\n",
    "    \n",
    "    #셀이 4개 # 셀 당 41개가 들어옴\n",
    "    enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell,\n",
    "                                            output_keep_prob = 0.5, seed=100)\n",
    "    otuputs , enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input, dtype = tf.float32)\n",
    "\n",
    "with tf.variable_scope('decode') :\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)   # 128\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell,output_keep_prob = 0.5)\n",
    "    # 2 x 41\n",
    "    # 앞의 인코더 망에서 출력된 states값(마지막 셀의 출력값==> 앞의 데이터를 모두 고려함(셀로 연결))\n",
    "\n",
    "    outputs , dec_stats = tf.nn.dynamic_rnn(dec_cell, dec_input, initial_state= enc_states, dtype = tf.float32)  \n",
    "    # initial_state로 인코드 망과 디코드망을 연결\n",
    "\n",
    "# output = 6 x 3(글자2개) x 128\n",
    "# n_class : 41\n",
    "# => 6x 3x 41\n",
    "model = tf.layers.dense(outputs, n_class, activation = None)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits = model, labels = targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'나무E'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data[1][1] + 'E'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0001 cost =  3.710885\n",
      "Epoch :  0002 cost =  2.686655\n",
      "Epoch :  0003 cost =  1.762030\n",
      "Epoch :  0004 cost =  1.124454\n",
      "Epoch :  0005 cost =  0.607119\n",
      "Epoch :  0006 cost =  0.341465\n",
      "Epoch :  0007 cost =  0.509761\n",
      "Epoch :  0008 cost =  0.282352\n",
      "Epoch :  0009 cost =  0.230448\n",
      "Epoch :  0010 cost =  0.212430\n",
      "Epoch :  0011 cost =  0.262437\n",
      "Epoch :  0012 cost =  0.154575\n",
      "Epoch :  0013 cost =  0.137292\n",
      "Epoch :  0014 cost =  0.249427\n",
      "Epoch :  0015 cost =  0.025318\n",
      "Epoch :  0016 cost =  0.167988\n",
      "Epoch :  0017 cost =  0.217812\n",
      "Epoch :  0018 cost =  0.191831\n",
      "Epoch :  0019 cost =  0.126363\n",
      "Epoch :  0020 cost =  0.203667\n",
      "Epoch :  0021 cost =  0.163226\n",
      "Epoch :  0022 cost =  0.056943\n",
      "Epoch :  0023 cost =  0.134108\n",
      "Epoch :  0024 cost =  0.074068\n",
      "Epoch :  0025 cost =  0.013083\n",
      "Epoch :  0026 cost =  0.034597\n",
      "Epoch :  0027 cost =  0.023425\n",
      "Epoch :  0028 cost =  0.009269\n",
      "Epoch :  0029 cost =  0.022525\n",
      "Epoch :  0030 cost =  0.018607\n",
      "Epoch :  0031 cost =  0.021008\n",
      "Epoch :  0032 cost =  0.006856\n",
      "Epoch :  0033 cost =  0.015346\n",
      "Epoch :  0034 cost =  0.002969\n",
      "Epoch :  0035 cost =  0.010462\n",
      "Epoch :  0036 cost =  0.004379\n",
      "Epoch :  0037 cost =  0.002167\n",
      "Epoch :  0038 cost =  0.008633\n",
      "Epoch :  0039 cost =  0.005785\n",
      "Epoch :  0040 cost =  0.002503\n",
      "Epoch :  0041 cost =  0.005095\n",
      "Epoch :  0042 cost =  0.002542\n",
      "Epoch :  0043 cost =  0.001585\n",
      "Epoch :  0044 cost =  0.002558\n",
      "Epoch :  0045 cost =  0.001520\n",
      "Epoch :  0046 cost =  0.002057\n",
      "Epoch :  0047 cost =  0.000477\n",
      "Epoch :  0048 cost =  0.001590\n",
      "Epoch :  0049 cost =  0.000883\n",
      "Epoch :  0050 cost =  0.001679\n",
      "Epoch :  0051 cost =  0.003374\n",
      "Epoch :  0052 cost =  0.002833\n",
      "Epoch :  0053 cost =  0.001888\n",
      "Epoch :  0054 cost =  0.004372\n",
      "Epoch :  0055 cost =  0.001057\n",
      "Epoch :  0056 cost =  0.001356\n",
      "Epoch :  0057 cost =  0.002105\n",
      "Epoch :  0058 cost =  0.001790\n",
      "Epoch :  0059 cost =  0.001169\n",
      "Epoch :  0060 cost =  0.000457\n",
      "Epoch :  0061 cost =  0.002455\n",
      "Epoch :  0062 cost =  0.000647\n",
      "Epoch :  0063 cost =  0.004125\n",
      "Epoch :  0064 cost =  0.000502\n",
      "Epoch :  0065 cost =  0.001875\n",
      "Epoch :  0066 cost =  0.001037\n",
      "Epoch :  0067 cost =  0.001086\n",
      "Epoch :  0068 cost =  0.001669\n",
      "Epoch :  0069 cost =  0.001838\n",
      "Epoch :  0070 cost =  0.002709\n",
      "Epoch :  0071 cost =  0.001503\n",
      "Epoch :  0072 cost =  0.000817\n",
      "Epoch :  0073 cost =  0.004697\n",
      "Epoch :  0074 cost =  0.002401\n",
      "Epoch :  0075 cost =  0.000588\n",
      "Epoch :  0076 cost =  0.001130\n",
      "Epoch :  0077 cost =  0.002077\n",
      "Epoch :  0078 cost =  0.000391\n",
      "Epoch :  0079 cost =  0.000521\n",
      "Epoch :  0080 cost =  0.002587\n",
      "Epoch :  0081 cost =  0.000364\n",
      "Epoch :  0082 cost =  0.001595\n",
      "Epoch :  0083 cost =  0.001991\n",
      "Epoch :  0084 cost =  0.000621\n",
      "Epoch :  0085 cost =  0.000722\n",
      "Epoch :  0086 cost =  0.000555\n",
      "Epoch :  0087 cost =  0.000653\n",
      "Epoch :  0088 cost =  0.001062\n",
      "Epoch :  0089 cost =  0.001969\n",
      "Epoch :  0090 cost =  0.000249\n",
      "Epoch :  0091 cost =  0.001138\n",
      "Epoch :  0092 cost =  0.000329\n",
      "Epoch :  0093 cost =  0.000525\n",
      "Epoch :  0094 cost =  0.000160\n",
      "Epoch :  0095 cost =  0.000550\n",
      "Epoch :  0096 cost =  0.000595\n",
      "Epoch :  0097 cost =  0.000312\n",
      "Epoch :  0098 cost =  0.000165\n",
      "Epoch :  0099 cost =  0.000729\n",
      "Epoch :  0100 cost =  0.000180\n"
     ]
    }
   ],
   "source": [
    "input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "for epoch in range(total_epoch) :  # 100개\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                      feed_dict = {enc_input: input_batch,\n",
    "                                  dec_input : output_batch,\n",
    "                                  targets : target_batch})\n",
    "    print('Epoch : ', '%04d' % (epoch+1),\n",
    "         'cost =  {:.6f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word -> 단어\n"
     ]
    }
   ],
   "source": [
    "def translate(word):\n",
    "    # 'P' : 모델의 자리수를 일치시키기 위해 그냥 넣음 (다른 의미 x )\n",
    "    seq_data = [word,'P' * len(word)] # word,PPPP\n",
    "    input_batch, output_batch, target_batch = make_batch([seq_data])\n",
    "    # 1x5(S또는E 추가)x41\n",
    "    prediction = tf.argmax(model,2)  # 면, 행, 열 중에서 열 중심  # 6x 3x 41 중 41에서 max값\n",
    "    result = sess.run(prediction,\n",
    "                     feed_dict={enc_input : input_batch,\n",
    "                               dec_input : output_batch,\n",
    "                               targets : target_batch})\n",
    "    decoded = [char_arr[i] for i in result[0]]   # 5에서 하나씩 뺌\n",
    "    end = decoded.index('E')\n",
    "    translated = ''.join(decoded[:end])\n",
    "\n",
    "    return translated\n",
    "print('word ->', translate('word'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR : Kakao API\n",
    "# https://developers.kakao.com/docs/restapi/vision#%EB%AC%B8%EC%9E%90-%EC%9D%B8%EC%8B%9D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.2.0.32-cp37-cp37m-win_amd64.whl (33.0 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\users\\ict01_18\\anaconda3\\envs\\tf_test\\lib\\site-packages (from opencv-python) (1.18.1)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.2.0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import requests\n",
    "import sys\n",
    "LIMIT_PX = 1024\n",
    "LIMIT_BYTE = 1024*1024\n",
    "LIMIT_BOX = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[224, 229, 228],\n",
       "        [225, 230, 229],\n",
       "        [225, 230, 229],\n",
       "        ...,\n",
       "        [218, 220, 221],\n",
       "        [217, 219, 220],\n",
       "        [217, 219, 220]],\n",
       "\n",
       "       [[224, 229, 228],\n",
       "        [225, 230, 229],\n",
       "        [225, 230, 229],\n",
       "        ...,\n",
       "        [219, 221, 222],\n",
       "        [218, 220, 221],\n",
       "        [218, 220, 221]],\n",
       "\n",
       "       [[225, 230, 229],\n",
       "        [225, 230, 229],\n",
       "        [226, 231, 230],\n",
       "        ...,\n",
       "        [220, 222, 223],\n",
       "        [220, 222, 223],\n",
       "        [219, 221, 222]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[230, 235, 234],\n",
       "        [230, 235, 234],\n",
       "        [230, 235, 234],\n",
       "        ...,\n",
       "        [222, 225, 229],\n",
       "        [222, 225, 229],\n",
       "        [222, 225, 229]],\n",
       "\n",
       "       [[229, 234, 233],\n",
       "        [229, 234, 233],\n",
       "        [230, 235, 234],\n",
       "        ...,\n",
       "        [222, 225, 229],\n",
       "        [222, 225, 229],\n",
       "        [222, 225, 229]],\n",
       "\n",
       "       [[227, 232, 231],\n",
       "        [228, 233, 232],\n",
       "        [229, 234, 233],\n",
       "        ...,\n",
       "        [223, 226, 230],\n",
       "        [223, 226, 230],\n",
       "        [224, 227, 231]]], dtype=uint8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = 'C:/Users/ICT01_18/hyeon/2.jpg'\n",
    "cv2.imread(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kakao_ocr_resize(image_path: str):  # 규격 이미지 사이즈\n",
    "\n",
    "    image = cv2.imread(image_path)         # 자기 이미지로 변경\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    if LIMIT_PX < height or LIMIT_PX < width:\n",
    "        ratio = float(LIMIT_PX) / max(height, width)\n",
    "        image = cv2.resize(image, None, fx=ratio, fy=ratio)\n",
    "        height, width, _ = height, width, _ = image.shape\n",
    "\n",
    "        # api 사용전에 이미지가 resize된 경우, recognize시 resize된 결과를 사용해야함.\n",
    "        image_path = \"{}_resized.jpg\".format(image_path)\n",
    "        cv2.imwrite(image_path, image)\n",
    "\n",
    "        return image_path\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "appkey = \"fefd26bbf235e38d765571b2be471226\"\n",
    "def kakao_ocr_detect(image_path: str, appkey: str):\n",
    "    \"\"\"\n",
    "    detect api request example\n",
    "    :param image_path: 이미지파일 경로\n",
    "    :param appkey: 카카오 앱 REST API 키\n",
    "    \"\"\"\n",
    "    API_URL = 'https://kapi.kakao.com/v1/vision/text/detect'\n",
    "\n",
    "    headers = {'Authorization': 'KakaoAK {}'.format(appkey)}\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    jpeg_image = cv2.imencode(\".jpg\", image)[1]\n",
    "    data = jpeg_image.tobytes()\n",
    "\n",
    "    return requests.post(API_URL, headers=headers, files={\"file\": data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kakao_ocr_recognize(image_path: str, boxes: list, appkey: str):\n",
    "\n",
    "    \"\"\"\n",
    "    recognize api request example\n",
    "    :param boxes: 감지된 영역 리스트. Canvas 좌표계: 좌상단이 (0,0) / 우상단이 (limit,0)\n",
    "                    감지된 영역중 좌상단 점을 기준으로 시계방향 순서, 좌상->우상->우하->좌하\n",
    "                    ex) [[[0,0],[1,0],[1,1],[0,1]], [[1,1],[2,1],[2,2],[1,2]], ...]\n",
    "    :param image_path: 이미지 파일 경로\n",
    "    :param appkey: 카카오 앱 REST API 키\n",
    "    \"\"\"\n",
    "    API_URL = 'https://kapi.kakao.com/v1/vision/text/recognize'\n",
    "\n",
    "    headers = {'Authorization': 'KakaoAK {}'.format(appkey)}\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    jpeg_image = cv2.imencode(\".jpg\", image)[1]\n",
    "    data = jpeg_image.tobytes()\n",
    "\n",
    "    return requests.post(API_URL, headers=headers, files={\"file\": data}, data={\"boxes\": json.dumps(boxes)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[detect] output:\n",
      "{'result': {'boxes': [[[139, 23], [362, 23], [362, 52], [139, 52]], [[386, 23], [451, 23], [451, 52], [386, 52]], [[32, 86], [139, 86], [139, 113], [32, 113]], [[175, 87], [425, 87], [425, 113], [175, 113]], [[145, 94], [157, 88], [165, 103], [153, 109]], [[30, 123], [140, 123], [140, 146], [30, 146]], [[148, 125], [161, 125], [161, 141], [148, 141]], [[175, 121], [439, 121], [439, 147], [175, 147]], [[129, 243], [160, 246], [157, 275], [126, 272]], [[144, 175], [194, 175], [194, 203], [144, 203]], [[158, 218], [179, 210], [189, 235], [167, 243]], [[167, 247], [183, 247], [183, 273], [167, 273]], [[196, 244], [421, 246], [421, 271], [196, 270]], [[196, 209], [336, 209], [336, 236], [196, 236]], [[200, 178], [311, 178], [311, 203], [200, 203]], [[321, 178], [378, 178], [378, 203], [321, 203]], [[353, 209], [462, 208], [463, 237], [353, 238]], [[397, 174], [461, 174], [461, 203], [397, 203]], [[5, 211], [157, 208], [159, 276], [7, 278]], [[8, 282], [119, 282], [119, 373], [8, 373]], [[200, 284], [309, 284], [309, 304], [200, 304]], [[339, 283], [522, 280], [523, 306], [339, 309]], [[200, 311], [282, 311], [282, 339], [200, 339]], [[199, 343], [323, 345], [323, 371], [198, 369]], [[394, 349], [507, 349], [507, 377], [394, 377]], [[8, 377], [91, 377], [91, 406], [8, 406]], [[105, 376], [161, 376], [161, 405], [105, 405]], [[199, 377], [365, 380], [365, 408], [198, 405]], [[5, 446], [142, 446], [142, 473], [5, 473]], [[158, 448], [226, 450], [226, 477], [157, 475]], [[253, 451], [352, 451], [352, 478], [253, 478]], [[540, 454], [592, 454], [592, 485], [540, 485]]]}}\n",
      "\n",
      "[recognize] output:\n",
      "{\n",
      "  \"result\": {\n",
      "    \"recognition_words\": [\n",
      "      \"\\uc2e0\\uc6a9\\uce74\\ub4dc\\ub9e4\\ucd9c\",\n",
      "      \"\\ub0b4 \\uc5ed\",\n",
      "      \"\\uce74\\ub4dc\\ubc88\\ud638\",\n",
      "      \"9440-0382-1745-****\",\n",
      "      \":\",\n",
      "      \"\\uce74 \\ub4dc \\uc0ac\",\n",
      "      \":\",\n",
      "      \"\\ube44\\uc528\\uce74\\ub4dc/IBK\\ube44\\uc528\\uc81c\\ud06c\",\n",
      "      \"\\ud638\",\n",
      "      \"[IBK\",\n",
      "      \":\",\n",
      "      \":\",\n",
      "      \"94400382****9857\",\n",
      "      \"2013-05-26\",\n",
      "      \"\\ube44\\uc528\\uccb4\\ud06c\",\n",
      "      \"\\uc2e0\\uc6a9\",\n",
      "      \"14:09:41\",\n",
      "      \"\\uc2b9\\uc778]\",\n",
      "      \"\\uc2ad\\ubc11\\uc715\\uc2eb\",\n",
      "      \"\\ucd9c \\uad6d \\ube44\",\n",
      "      \"\\u00f89YYU460\",\n",
      "      \"\\uc54c\\ubb34\\uac1c\\uc6d4 :00\",\n",
      "      \"16,000\",\n",
      "      \"781299839\",\n",
      "      \"\\ube44\\uc528\\uce74\\ub4dc\",\n",
      "      \"\\uc0ac\\uc5c5\\uc790\",\n",
      "      \"\\ubc88\\ud638\",\n",
      "      \"616-28-83377\",\n",
      "      \"2013-05-26\",\n",
      "      \"14:09\",\n",
      "      \"CASHIER\",\n",
      "      \"\\uc9c1\\uc6d0\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-6c78391ea15b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# 자동 가계부로 사용가능\n",
    "\n",
    "def main():\n",
    "    image_path = 'C:/Users/ICT01_18/hyeon/2.jpg'\n",
    "    appkey = \"fefd26bbf235e38d765571b2be471226\"\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Please run with args: $ python example.py /path/to/image appkey\")\n",
    "    #image_path, appkey = sys.argv[1], sys.argv[2]\n",
    "\n",
    "    resize_impath = kakao_ocr_resize(image_path)\n",
    "    if resize_impath is not None:\n",
    "        image_path = resize_impath\n",
    "        print(\"원본 대신 리사이즈된 이미지를 사용합니다.\")\n",
    "\n",
    "    output = kakao_ocr_detect(image_path, appkey).json()\n",
    "    print(\"[detect] output:\\n{}\\n\".format(output))\n",
    "\n",
    "    boxes = output[\"result\"][\"boxes\"]\n",
    "    boxes = boxes[:min(len(boxes), LIMIT_BOX)]\n",
    "    output = kakao_ocr_recognize(image_path, boxes, appkey).json()\n",
    "    print(\"[recognize] output:\\n{}\\n\".format(json.dumps(output, sort_keys=True, indent=2)))\n",
    "\n",
    "output = main()\n",
    "output['result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ICT01_18\\\\Anaconda3\\\\envs\\\\tf_test\\\\lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
       " '-f',\n",
       " 'C:\\\\Users\\\\ICT01_18\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-dd707d51-cc5f-40a6-9512-6388d31d751e.json']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recognition_words': ['신용카드매출',\n",
       "  '내 역',\n",
       "  '카드번호',\n",
       "  '9440-0382-1745-****',\n",
       "  ':',\n",
       "  '카 드 사',\n",
       "  ':',\n",
       "  '비씨카드/IBK비씨제크',\n",
       "  '호',\n",
       "  '[IBK',\n",
       "  ':',\n",
       "  ':',\n",
       "  '94400382****9857',\n",
       "  '2013-05-26',\n",
       "  '비씨체크',\n",
       "  '신용',\n",
       "  '14:09:41',\n",
       "  '승인]',\n",
       "  '슭밑윕싫',\n",
       "  '출 국 비',\n",
       "  'ø9YYU460',\n",
       "  '알무개월 :00',\n",
       "  '16,000',\n",
       "  '781299839',\n",
       "  '비씨카드',\n",
       "  '사업자',\n",
       "  '번호',\n",
       "  '616-28-83377',\n",
       "  '2013-05-26',\n",
       "  '14:09',\n",
       "  'CASHIER',\n",
       "  '직원']}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install opencv-python\n",
    "import json\n",
    "import cv2 # computer vision tool (이미지, 동영상 처리)\n",
    "import requests # html 페이지 요청\n",
    "import sys\n",
    "LIMIT_PX = 1024\n",
    "LIMIT_BYTE = 1028*1024\n",
    "LIMIT_BOX = 40\n",
    "# OCR 절차\n",
    "# 이미지 사이즈 조정\n",
    "# 글씨를 둘러싼 Box를 detection\n",
    "\n",
    "def kakao_ocr_resize(image_path: str):\n",
    "    image = cv2.imread('2.jpg')\n",
    "    height, width, _ = image.shape\n",
    "    if LIMIT_PX < height or LIMIT_PX < width:\n",
    "        ratio = float(LIMIT_PX) / max(heigth, width)\n",
    "        image = cv2.resize(image, None, fx=ratio, fy=ratio)\n",
    "        height, width, _ = height, width, _ = image.shape\n",
    "        image_path = \"{}_resize.jpg\".format('2')\n",
    "        cv2.imwrite(image_path, image)\n",
    "        return image_path\n",
    "    return None\n",
    "\n",
    "# 사용법\n",
    "# open api 함수를 원격호출 (remote call)\n",
    "def kakao_ocr_detect(image_path: str, appkey: str):\n",
    "    API_URL ='https://kapi.kakao.com/v1/vision/text/detect'\n",
    "    headers = {'Authorization': 'KakaoAK {}'.format('7d278c16a4138b64cf7e96eb6fb432a4')}\n",
    "    image = cv2.imread('2.jpg') # 이미지 로드\n",
    "    # naver는 jpg만, gif 애니메이션 지원, png 투명도 지원\n",
    "    jpeg_image = cv2.imencode('.jpg', image)[1]\n",
    "    data = jpeg_image.tobytes() # 네크워크 전송 -> serialization\n",
    "    # 네트워크일 때 => 데이터가 시리얼로 가야 함 (모뎀)\n",
    "    # 8개 라인으로 구성, 4개 회선만 사용 (1개 전송, 1개 수신)\n",
    "    # 네트워크를 통해 함수 호출 (시간) : 비동기 방식\n",
    "    # 동기 방식(호출 후 돌아올 때까지 기다림), 비동기 방식(다른 작업 중 -event 받아서 작업)\n",
    "    # 대표적인 비동기 통신 방식이 AJAX : 웹 브라우저 내부적 처리\n",
    "    # 인터넷 표준 데이터 형식 json\n",
    "    return requests.post(API_URL, headers=headers, files={\"file\": data})\n",
    "\n",
    "def kakao_ocr_recognize(image_path: str, boxes: list, appkey: str):\n",
    "    API_URL = 'https://kapi.kakao.com/v1/vision/text/recognize'\n",
    "    headers = {'Authorization': 'KakaoAK {}'.format('7d278c16a4138b64cf7e96eb6fb432a4')}\n",
    "    image = cv2.imread('2.jpg')\n",
    "    jpeg_image = cv2.imencode('.jpg', image)[1]\n",
    "    data = jpeg_image.tobytes()\n",
    "    # 인식은 BOX로 텍스트 구역을 확인하고, recognize 인식한다.\n",
    "    return requests.post(API_URL, headers=headers, files={\"file\": data}, data={\"boxes\": json.dumps(boxes)})\n",
    "\n",
    "def main():\n",
    "    image_path, appkey = sys.argv[1], sys.argv[2]\n",
    "    resize_impath = kakao_ocr_resize(image_path)\n",
    "    output = kakao_ocr_detect(image_path, appkey).json()\n",
    "    boxes = output['result']['boxes']\n",
    "    boxes = boxes[:min(len(boxes), LIMIT_BOX)]\n",
    "    output = kakao_ocr_recognize(image_path, boxes, appkey).json()\n",
    "    return output\n",
    "output = main()\n",
    "output['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\ICT01_18\\\\Anaconda3\\\\envs\\\\tf_test\\\\lib\\\\site-packages\\\\ipykernel_launcher.py',\n",
       " '-f',\n",
       " 'C:\\\\Users\\\\ICT01_18\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-dd707d51-cc5f-40a6-9512-6388d31d751e.json']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 읽어오기\n",
    "# 배경 제거 전\n",
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture('corrupted_video.mp4')   # 위 경로에에서 video 캡처 명령\n",
    "if (cap.isOpened() == False) : # 파일이 없거나 장치가 없을 때\n",
    "    print(\"Error 발생\")\n",
    "\n",
    "while(cap.isOpened()) :\n",
    "    ret, frame = cap.read()  \n",
    "    # 1개의 frame을 읽어옴( 동영상은 ntsc 방식으로 초당 29.7프레임)\n",
    "    \n",
    "    if ret == True:\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): # q를 누르면 종료\n",
    "            break\n",
    "    else : \n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 읽어오기\n",
    "# 배경 제거\n",
    "import cv2\n",
    "import numpy as np\n",
    "history = 30\n",
    "cap = cv2.VideoCapture('corrupted_video.mp4')   # 위 경로에에서 video 캡처 명령\n",
    "if (cap.isOpened() == False) : # 파일이 없거나 장치가 없을 때\n",
    "    print(\"Error 발생\")\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()  # 배경 필터(background filter)\n",
    "while(cap.isOpened()) :\n",
    "    ret, frame = cap.read()  \n",
    "    # 1개의 frame을 읽어옴( 동영상은 ntsc 방식으로 초당 29.7프레임)\n",
    "    \n",
    "    if ret == True:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        fgmask = fgbg.apply(gray, learningRate = 1.0/history)\n",
    "        cv2.imshow('Frame', fgmask)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'): # q를 누르면 종료\n",
    "            break\n",
    "    else : \n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 저장하기\n",
    "# 동영상 데이터의 전처리 => AI model에 훈련 => 실시간 인식\n",
    "# \n",
    "import numpy as np\n",
    "import cv2 #computer vision ==> 사운드가 안됨\n",
    "\n",
    "cap = cv2.VideoCapture('corrupted_video.mp4') \n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0 , (640,480)) # 윈도우 표준 형식 avi\n",
    "while(cap.isOpened()) :\n",
    "    ret, frame = cap.read()   # embeded 장치에 활용(차선인식 등)\n",
    "    if ret == True:\n",
    "        frame = cv2.flip(frame,0)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # 고속으로 가능( 흑백 처리- 생략 가능 )\n",
    "        out.write(frame)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else :\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# AE(Auto Encoder )\n",
    "# denoising autoencoder\n",
    "# 원본데이터, noise가 있는 원본 데이터 \n",
    "# 학습은 noise가 있는 학습 데이터 => 가중치\n",
    "# 노이즈가 있는 데이터를 원본데이터로 출력함\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def vis(images, save_name):\n",
    "    dim = images.shape[0]\n",
    "    n_image_rows = int(np.ceil(np.sqrt(dim)))\n",
    "    n_image_cols = int(np.ceil(dim* 1.0/n_image_rows))\n",
    "    gs = gridspec.GridSpec(n_image_rows, n_image_cols, top = 1., bottom= 0.,\n",
    "                          right = 1., left = 0., hspace = 0., wspace = 0.)\n",
    "    for g, count in zip(gs, range(int(dim))):\n",
    "        ax = plt.subplot(g)\n",
    "        ax.imshow(images[count,:].reshape((28,28)))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.savefig(save_name + '_vis.png')\n",
    "    plt.show()\n",
    "mnist_width = 28\n",
    "n_visible = mnist_width * mnist_width\n",
    "n_hidden = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption_level = 0.3\n",
    "# 784\n",
    "X = tf.placeholder('float', [None, n_visible], name = 'X')\n",
    "mask = tf.placeholder('float', [None, n_visible], name = 'mask')\n",
    "\n",
    "W_init_max = 4 * np.sqrt(6./ (n_visible + n_hidden))\n",
    "# 균등분포에서 데이터 생성( 가중치 초기화)\n",
    "W_init = tf.random_uniform(shape = [n_visible, n_hidden],\n",
    "                          minval = -W_init_max, maxval = W_init_max)\n",
    "\n",
    "W = tf.Variable(W_init, name = 'W')\n",
    "b = tf.Variable(tf.zeros([n_hidden]), name = 'b')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_prime = tf.transpose(W) # 자기자신의 전치\n",
    "b_prime = tf.Variable(tf.zeros([n_visible]), name = 'b_prime')\n",
    "def model(X, mask, W,b, W_prime, b_prime):\n",
    "    tilde_X  = mask * X  # 행렬 요소곱\n",
    "    Y = tf.nn.sigmoid(tf.matmul(tilde_X,W) + b)\n",
    "    Z = tf.nn.sigmoid(tf.matmul(Y, W_prime) + b_prime)\n",
    "    return Z\n",
    "\n",
    "Z = model(X,mask, W,b , W_prime , b_prime)\n",
    "\n",
    "# 가중치에 이미지의 특성이 추출됨\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d7160e004861>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# X : 원본데이터(깔끔한 이미지) / Z : 예측된 데이터( 자기자신(잡티가 있는 이미지))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# 오토인코더는 인풋과 아웃풋이 모두 자기 자신\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 비지도 학습 => 데이터의 특징을 추출함\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#  인풋데이터ㅇ에 노이즈를 부여하여 이미지를 부분 부분 없앰 => 학습하면 원래의 데이터가 나옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.pow(X-Z, 2))  # X : 원본데이터(깔끔한 이미지) / Z : 예측된 데이터( 자기자신(잡티가 있는 이미지))\n",
    "# 오토인코더는 인풋과 아웃풋이 모두 자기 자신\n",
    "# 비지도 학습 => 데이터의 특징을 추출함\n",
    "#  인풋데이터ㅇ에 노이즈를 부여하여 이미지를 부분 부분 없앰 => 학습하면 원래의 데이터가 나옴\n",
    "\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(0.02).minimize(cost)\n",
    "predict_op = Z\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2588214\n",
      "1 0.22730303\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100) :\n",
    "        for start, end in zip(range(0, len(trX), 128),\n",
    "                             range(128, len(trX)+1, 128)):\n",
    "            input_ = trX[start:end]\n",
    "            # 이항분포로 마스크 생성( 원본이미지에 노이즈를 주기 위해 ) => \n",
    "            mask_up = np.random.binomial(1,1-corruption_level,\n",
    "                                        input_.shape)\n",
    "            sess.run(train_op, feed_dict = {X:input_, mask : mask_up})\n",
    "        mask_up = np.random.binomial(1,1 - corruption_level,\n",
    "                                    teX.shape)\n",
    "        print(i, sess.run(cost, feed_dict={X : teX, mask : mask_up}))\n",
    "    mask_np = np.random.binomial(1,1-corruption_level,\n",
    "                                teX[:100].shape)\n",
    "    \n",
    "    # 예측할 때 이미지에 마스크\n",
    "    predicted_imgs = sess.run(predict_op, feed_dict={X:teX[:100],\n",
    "                                                    mask : mask_up})\n",
    "    input_imgs = teX[:100]\n",
    "vis(predicted_img, 'pred') # 예측된 이미지\n",
    "vis(input_imgs, 'in')  # 입력 이미지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vaiational autoencoder ( VAE)\n",
    "# generative model 생성모델\n",
    "#  * 판별모델(discrimitive model) : 선을 그어서 분류를 하는 모델( tree)\n",
    "#  * 생성모델 : 확률을 찾아냄-> 평균, 분산 고려( 분포 )\n",
    "#     - 잠재변수latent variables : 이미지를 보고 이미지의 특징만 잡아냄(키, 성별 등)\n",
    "#                                 => 특징을 바탕으로 새로운 이미지 만들어 낼 수 있음(생성모델)\n",
    "#                                - 특징을 뽑아"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'mask_4' with dtype float and shape [?,784]\n\t [[node mask_4 (defined at <ipython-input-10-61a6aa8cc4f9>:4) ]]\n\nOriginal stack trace for 'mask_4':\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-61a6aa8cc4f9>\", line 4, in <module>\n    mask = tf.placeholder('float', [None, n_visible], name = 'mask')\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6261, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'mask_4' with dtype float and shape [?,784]\n\t [[{{node mask_4}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3600e1d2ab9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0minput_\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mmask_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mcorruption_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmask_np\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mmask_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcorruption_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mteX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmask_np\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'mask_4' with dtype float and shape [?,784]\n\t [[node mask_4 (defined at <ipython-input-10-61a6aa8cc4f9>:4) ]]\n\nOriginal stack trace for 'mask_4':\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\asyncio\\base_events.py\", line 538, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\asyncio\\base_events.py\", line 1782, in _run_once\n    handle._run()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-10-61a6aa8cc4f9>\", line 4, in <module>\n    mask = tf.placeholder('float', [None, n_visible], name = 'mask')\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 6261, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\ICT01_18\\Anaconda3\\envs\\tf_test\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "def vis(images, save_name):\n",
    "    dim = images.shape[0]\n",
    "    n_image_rows = int(np.ceil(np.sqrt(dim)))\n",
    "    n_image_cols = int(np.ceil(dim* 1.0/n_image_rows))\n",
    "    gs = gridspec.GridSpec(n_image_rows, n_image_cols, top = 1., bottom= 0.,\n",
    "                          right = 1., left = 0., hspace = 0., wspace = 0.)\n",
    "    for g, count in zip(gs, range(int(dim))):\n",
    "        ax = plt.subplot(g)\n",
    "        ax.imshow(images[count,:].reshape((28,28)))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.savefig(save_name + '_vis.png')\n",
    "    plt.show()\n",
    "mnist_width = 28\n",
    "n_visible = mnist_width * mnist_width\n",
    "n_hidden = 500\n",
    "\n",
    "corruption_level = 0.3\n",
    "X = tf.placeholder(\"float\", [None, n_visible], name \n",
    "                   \n",
    "                   = 'X')\n",
    "mask = tf.placeholder(\"float\", [None, n_visible], name = 'mask')\n",
    "W_init_max = 4 * np.sqrt(6. / (n_visible+ n_hidden))\n",
    "W_init = tf.random_uniform(shape = [n_visible, n_hidden], minval = -W_init_max,\n",
    "                          maxval = W_init_max)\n",
    "W = tf.Variable(W_init, name ='W')\n",
    "b = tf.Variable(tf.zeros([n_hidden]), name = 'b')\n",
    "\n",
    "W_prime = tf.transpose(W)\n",
    "b_prime = tf.Variable(tf.zeros([n_visible]), name = 'b_prime')\n",
    "def model(X, mask, W, b, W_prime, b_prime):\n",
    "    tilde_X = mask * X\n",
    "    Y = tf.nn.sigmoid(tf.matmul(tilde_X, W)+ b)\n",
    "    Z = tf.nn.sigmoid(tf.matmul(Y, W_prime) + b_prime)\n",
    "    return Z\n",
    "Z = model(X, mask, W, b, W_prime, b_prime)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100):\n",
    "        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128 )):\n",
    "            input_= trX[start:end]\n",
    "            mask_np = np.random.binomial(1,1-corruption_level, input_.shape)\n",
    "            sess.run(train_op, feed_dict = {X:input_, mask:mask_np})\n",
    "        mask_np = np.random.binomial(1,1 - corruption_level, teX.shape)\n",
    "        print(i, sess.run(cost, feed_dict = {X:teX, mask:mask_np}))\n",
    "    mask_np = np.random.binomial(1, 1 -corruption_level, teX[:100].shape)\n",
    "    predicted_imgs =sess.run(predict_op, feed_dict = {X: teX[:100], mask:mask_np})\n",
    "    input_imgs = teX[:100]\n",
    "vis(predicted_imgs, 'pred')\n",
    "vis(input_imgs, 'in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "ops.reset_default_graph()\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "train_x , train_y = train\n",
    "from functools import partial\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)\n",
    "\n",
    "n_inputs = 28*28\n",
    "n_hidden1 = 500\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 20\n",
    "n_hidden4 = n_hidden2\n",
    "n_hidden5 = n_hidden1\n",
    "n_outputs = n_inputs  # 입력이 출력으로\n",
    "learning_rate = 0.001\n",
    "# without scaling 초기화( 스케일링 없이 초기화 )\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x00000254357C2B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438888CC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438931D88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438931D88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438931D88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000025438931D88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002543F15A6C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "my_dense_layer = partial(   # 매개변수 일부를 고정 => 상수취급\n",
    "    tf.layers.dense,\n",
    "    activation = tf.nn.elu,  # Exponential linear Unit ( -1을 고려한 relu)\n",
    "    kernel_initializer = initializer)\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "hidden1 = my_dense_layer(X, n_hidden1)\n",
    "hidden2 = my_dense_layer(hidden1, n_hidden2)\n",
    "hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation = None)\n",
    "hidden3_sigma = my_dense_layer(hidden2, n_hidden3, activation = None)\n",
    "noise = tf.random_normal(tf.shape(hidden3_sigma), dtype = tf.float32)\n",
    "# 평균 시그마(분산) 노이즈  => 변동된 특성을 추출하기 위함 ( 요소곱 )\n",
    "hidden3 = hidden3_mean + hidden3_sigma * noise\n",
    "\n",
    "hidden4 = my_dense_layer(hidden3, n_hidden4)\n",
    "hidden5 = my_dense_layer(hidden4, n_hidden5)\n",
    "logits = my_dense_layer(hidden5, n_outputs, activation= None)\n",
    "outputs = tf.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = X, logits = logits)\n",
    "\n",
    "# 재구축 손실\n",
    "reconstruction_loss = tf.reduce_sum(xentropy)\n",
    "\n",
    "eps = 1e-10 # 앱실론(epsilon)상수 : 로그의 성질 -- 0이면 무한대 => 무한대로 가는 것을 막기 위함\n",
    "# 해당 앱실론 값 이하의 차이면 같은 것으로 간주\n",
    "\n",
    "# cost function : log liklihood (우도 확률)을 고려한 식 \n",
    "# 지연손실(확률 분포 곡선 )\n",
    "latent_loss = 0.5 * tf.reduce_mean( \n",
    "    tf.square(hidden3_sigma) + tf.square(hidden3_mean)- 1 - tf.log(eps + tf.square(hidden3_sigma)))\n",
    "\n",
    "loss = reconstruction_loss + latent_loss\n",
    "optimizer=  tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "training_op= optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train total loss:  12777.681 \tReconstruction loss:  12773.199 \tLatent loss :  4.481745\n",
      "1 Train total loss:  12188.6455 \tReconstruction loss:  12183.971 \tLatent loss :  4.6745887\n",
      "2 Train total loss:  11055.033 \tReconstruction loss:  11050.315 \tLatent loss :  4.7176256\n",
      "3 Train total loss:  10117.8125 \tReconstruction loss:  10112.883 \tLatent loss :  4.9295235\n",
      "49% Train total loss:  10697.864 \tReconstruction loss:  10692.973 \tLatent loss :  4.8915434\n",
      "5 Train total loss:  10431.455 \tReconstruction loss:  10426.429 \tLatent loss :  5.0265617\n",
      "6 Train total loss:  10381.528 \tReconstruction loss:  10376.626 \tLatent loss :  4.9027863\n",
      "7 Train total loss:  10219.271 \tReconstruction loss:  10214.192 \tLatent loss :  5.078694\n",
      "8 Train total loss:  9916.226 \tReconstruction loss:  9911.135 \tLatent loss :  5.091222\n",
      "9 %Train total loss:  9235.441 \tReconstruction loss:  9230.321 \tLatent loss :  5.120447\n",
      "10 Train total loss:  9755.961 \tReconstruction loss:  9750.6875 \tLatent loss :  5.273396\n",
      "11 Train total loss:  10138.351 \tReconstruction loss:  10133.068 \tLatent loss :  5.2825027\n",
      "12 Train total loss:  9543.015 \tReconstruction loss:  9537.571 \tLatent loss :  5.4430127\n",
      "13 Train total loss:  9465.837 \tReconstruction loss:  9460.412 \tLatent loss :  5.4251285\n",
      "14 Train total loss:  9719.298 \tReconstruction loss:  9713.847 \tLatent loss :  5.4513774\n",
      "15 Train total loss:  9318.617 \tReconstruction loss:  9313.054 \tLatent loss :  5.563612\n",
      "16% Train total loss:  9822.147 \tReconstruction loss:  9816.637 \tLatent loss :  5.5108757\n",
      "17 Train total loss:  9244.118 \tReconstruction loss:  9238.582 \tLatent loss :  5.5360217\n",
      "18 Train total loss:  9608.872 \tReconstruction loss:  9603.1 \tLatent loss :  5.772582\n",
      "19 Train total loss:  9411.088 \tReconstruction loss:  9405.1875 \tLatent loss :  5.9004674\n",
      "20 Train total loss:  9544.319 \tReconstruction loss:  9538.197 \tLatent loss :  6.1225567\n",
      "21 Train total loss:  9284.887 \tReconstruction loss:  9278.916 \tLatent loss :  5.9711857\n",
      "22 Train total loss:  9193.79 \tReconstruction loss:  9187.694 \tLatent loss :  6.0958886\n",
      "23 Train total loss:  9502.829 \tReconstruction loss:  9496.623 \tLatent loss :  6.206534\n",
      "24 Train total loss:  9352.06 \tReconstruction loss:  9345.912 \tLatent loss :  6.14714\n",
      "25 Train total loss:  8870.766 \tReconstruction loss:  8864.53 \tLatent loss :  6.235548\n",
      "26 Train total loss:  8985.082 \tReconstruction loss:  8978.779 \tLatent loss :  6.302264\n",
      "27% Train total loss:  9441.745 \tReconstruction loss:  9435.541 \tLatent loss :  6.2045236\n",
      "28 Train total loss:  9020.935 \tReconstruction loss:  9014.385 \tLatent loss :  6.5496206\n",
      "29 Train total loss:  9355.072 \tReconstruction loss:  9348.461 \tLatent loss :  6.611306\n",
      "30% Train total loss:  9290.805 \tReconstruction loss:  9284.382 \tLatent loss :  6.4224057\n",
      "31 Train total loss:  9230.96 \tReconstruction loss:  9223.945 \tLatent loss :  7.01497\n",
      "32 Train total loss:  9274.028 \tReconstruction loss:  9267.218 \tLatent loss :  6.8103886\n",
      "33 Train total loss:  8798.753 \tReconstruction loss:  8792.053 \tLatent loss :  6.6997747\n",
      "34 Train total loss:  9368.341 \tReconstruction loss:  9361.579 \tLatent loss :  6.7615795\n",
      "35 Train total loss:  9548.55 \tReconstruction loss:  9541.818 \tLatent loss :  6.7313924\n",
      "36 Train total loss:  9170.44 \tReconstruction loss:  9163.473 \tLatent loss :  6.968053\n",
      "37 Train total loss:  9087.187 \tReconstruction loss:  9080.321 \tLatent loss :  6.865029\n",
      "38 Train total loss:  9139.523 \tReconstruction loss:  9132.535 \tLatent loss :  6.9881043\n",
      "39 Train total loss:  9253.27 \tReconstruction loss:  9246.179 \tLatent loss :  7.0906463\n",
      "40 Train total loss:  8995.519 \tReconstruction loss:  8988.496 \tLatent loss :  7.0222683\n",
      "41 Train total loss:  9047.227 \tReconstruction loss:  9040.017 \tLatent loss :  7.2100363\n",
      "42 Train total loss:  9018.712 \tReconstruction loss:  9011.318 \tLatent loss :  7.3932123\n",
      "43 Train total loss:  8680.998 \tReconstruction loss:  8673.551 \tLatent loss :  7.4469624\n",
      "44 Train total loss:  9061.935 \tReconstruction loss:  9054.361 \tLatent loss :  7.5728383\n",
      "45% Train total loss:  9226.173 \tReconstruction loss:  9218.684 \tLatent loss :  7.4887805\n",
      "46 Train total loss:  9017.915 \tReconstruction loss:  9010.506 \tLatent loss :  7.4089856\n",
      "47 Train total loss:  8838.581 \tReconstruction loss:  8830.92 \tLatent loss :  7.6612525\n",
      "48 Train total loss:  9010.6045 \tReconstruction loss:  9002.811 \tLatent loss :  7.793585\n",
      "49 Train total loss:  9208.124 \tReconstruction loss:  9200.436 \tLatent loss :  7.688744\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "n_digis = 60\n",
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs) :\n",
    "        n_batches = mnist.train.num_examples // batch_size\n",
    "        for iteration in range(n_batches):\n",
    "            print(\"\\r{}%\".format(100 * iteration // n_batches),\n",
    "                 end = '')\n",
    "            sys.stdout.flush()\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X:X_batch})\n",
    "        loss_val , reconstruction_loss_val, latent_loss_val = sess.run([loss, reconstruction_loss , latent_loss], feed_dict={X:X_batch})\n",
    "        print(\"\\r{}\".format(epoch), 'Train total loss: ', loss_val, '\\tReconstruction loss: ',\n",
    "             reconstruction_loss_val, '\\tLatent loss : ', latent_loss_val)\n",
    "        codings_rnd = np.random.normal(size = [n_digis, n_hidden3])\n",
    "        outputs_val = outputs.eval(feed_dict={hidden3: codings_rnd})\n",
    "        \n",
    "# 가중치를 통과한 데이터 ( tset ) : 모델 완성\n",
    "# PCA처럼 비지도학습의 특성으로 추출된 데이터 ( 분류와 예측이 잘되기 위해서 함 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
